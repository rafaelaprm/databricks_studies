{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do I get with PySpark?\n",
    "Apache Spark is a fast and general engine for large-scale data processing.\n",
    "\n",
    "Gain\n",
    "- Work with big data\n",
    "- Native SQL\n",
    "- Decent documentation\n",
    "Lose\n",
    "- Amazing documentation\n",
    "- Easy plotting\n",
    "- Indices\n",
    "\n",
    "### Primer\n",
    "Distributed compute\n",
    "- YARN, Mesos, Standalone cluster\n",
    "Abstractions\n",
    "- RDD -- distributed colletion of objects\n",
    "- Dataframe -- distributed dataset of tabular data\n",
    "    - Integrated SQL\n",
    "    - ML algorithms\n",
    "\n",
    "### Important concepts\n",
    "Immutable\n",
    "- changes create new object references\n",
    "- Old versions are unchanged\n",
    "\n",
    "Lazy\n",
    "- Compute does not happen until output is requested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df = pd.read_csv(\"caminho\")\n",
    "\n",
    "# PySpark\n",
    "df = spark.read.options(header=True, inferSchema=True).csv(\"caminho\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df\n",
    "df.head(10)\n",
    "\n",
    "# PySpark\n",
    "df.show()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and Pyspark\n",
    "\n",
    "df.columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df.columns = ['a', 'b', 'c']\n",
    "df.rename(columns={'old': 'new'})\n",
    "\n",
    "#PySpark\n",
    "df.toDF('a', 'b', 'c')\n",
    "df.withColumnRenamed('old', 'new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df.drop('mpg', axis=1)\n",
    "\n",
    "#PySpark\n",
    "df.drop('mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and PySpark\n",
    "\n",
    "df[df.mpg < 30]\n",
    "df[(df.mpg < 30) & (df.cyl > 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas 1/0 = inf\n",
    "df['gpm'] = 1 / df.mpg\n",
    "\n",
    "#PySpark 1/0 = null\n",
    "df.withColumn('gpm', 1 / df.mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df.fillna(0) # <-- Many more options\n",
    "\n",
    "#PySpark\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and PySpark\n",
    "df.groupby(['cyl', 'gear']).agg({'mpg': mean, 'disp': min})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "import numpy as np\n",
    "df['logdisp'] = np.log(df.disp)\n",
    "\n",
    "#PySpark\n",
    "import pyspark.sql.functions as F\n",
    "df.withColumn('logdisp', F.log(df.disp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row conditional statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df['cond'] = df.apply(lambda r: 1 if r.mpg > 20 else 2 if r.cyl == 6 else 3, axis=1)\n",
    "\n",
    "# PySpark\n",
    "df.withColumn('cond', F.when(df.mpg > 20, 1).when(df.cyl == 6, 2).otherwise(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df['disp1'] = df.disp.apply(lambda x: x+1)\n",
    "\n",
    "# PySpark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "fn = F.udf(lambda x: x+1, DoubleType()) #udf user-defined function with he type\n",
    "df.withColumn('disp1', fn(df.disp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge/Join dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "left.merge(right, on='key')\n",
    "left.merg(right, left_on='a', right_on='b')\n",
    "\n",
    "#if you want to join and your left and right keys has a little bit different name\n",
    "# PySpark\n",
    "left.join(right, on='key')\n",
    "left.join(right, left.a == right.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'], aggfunc=np.sum)\n",
    "\n",
    "# PySpark\n",
    "pd.groubBy('A', 'B').pivot('C').sum('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df.describe()\n",
    "\n",
    "# PySpark\n",
    "df.describe().show() #(only count, mean, stddev, min, max)\n",
    "# quartiles\n",
    "df.selectExpr(\n",
    "        \"percentile_approx(mpg, array(.25, .5, .75)) as mpg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas \n",
    "df.hist()\n",
    "\n",
    "#PySpark\n",
    "df.sample(False, 0.1).toPandas().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark\n",
    "df.createOrReplaceTempView('foo')\n",
    "df2 =  spark.sql('select * from foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure to\n",
    "- Use pyspark.sql.functions and other built in funcions\n",
    "- Use the same version of python and packages on cluster as driver\n",
    "- Check out the UI at http://localhost:4040/\n",
    "- Learn about SSH por forwarding\n",
    "- Check out Spark MLlib\n",
    "- RTFM: https://spark.apache.org/docs/latest\n",
    "\n",
    "## Things not to do\n",
    "- Try to iterate through rows\n",
    "- Hard code a master in your driver\n",
    "    - Use spark-submit for that\n",
    "- df.toPandas().head()\n",
    "    - instead do: df.limit(5).toPandas()\n",
    "\n",
    "## If things go wrong\n",
    "- Google it\n",
    "- Search/Ask stack Overflow (tag apache-spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
