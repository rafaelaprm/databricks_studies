{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling with PySpark for Data Scientists Who Know Pandas\n",
    "[Data Wrangling with PySpark for Data Scientists Who Know Pandas - Andrew Ray](https://www.youtube.com/watch?v=XrpSRCwISdk&t=10s&ab_channel=Databricks)\n",
    "\n",
    "and [Azure Databricks using Python with PySpark](https://www.youtube.com/watch?v=qYis56u8w4U&list=PLyvglJJUQCYE_M4W65MxNynWVm73SH9R4&index=8&t=1725s&ab_channel=BryanCafferky)\n",
    "\n",
    "## What do I get with PySpark?\n",
    "Apache Spark is a fast and general engine for large-scale data processing.\n",
    "\n",
    "YOU DON'T BRING THE DATA TO THE PROGRAM, YOU BRING THE PROGRAM TO THE DATA\n",
    "\n",
    "Gain\n",
    "- Work with big data\n",
    "- Native SQL\n",
    "- Decent documentation\n",
    "\n",
    "Lose\n",
    "- Amazing documentation\n",
    "- Easy plotting\n",
    "- Indices\n",
    "\n",
    "### Primer\n",
    "Distributed compute\n",
    "- YARN, Mesos, Standalone cluster\n",
    "Abstractions\n",
    "- RDD -- distributed colletion of objects\n",
    "- Dataframe -- distributed dataset of tabular data\n",
    "    - Integrated SQL\n",
    "    - ML algorithms\n",
    "\n",
    "### Important concepts\n",
    "Immutable\n",
    "- Changes create new object references\n",
    "- Old versions are unchanged\n",
    "\n",
    "Lazy\n",
    "- Compute does not happen until output is requested\n",
    "\n",
    "### Cluster Architecture\n",
    "- Driver runs the users main function and executes the various parallel oerations on the worker nodes\n",
    "- The results od the operations are collected by the driver\n",
    "- The worker nodes read and write data from/to Data Sources including HDFS, SQL\n",
    "- Worker node also cache transformed data in memory as RDDs (Resilient Data Sets)\n",
    "- Worker nodes and Driver Node execute as VMs in public clouds (AWS, Google and Azure)\n",
    "- Nodes run JVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df = pd.read_csv(\"caminho\")\n",
    "\n",
    "# PySpark\n",
    "df = spark.read.options(header=True, inferSchema=True).csv(\"caminho\")\n",
    "#or\n",
    "df = spark.read.csv(\"caminho\", header=True, sep=',', inferSchema=True)\n",
    "#or\n",
    "df = spark.read.format(\"parquet\").load(\"caminho\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "df.write.format(\"parquet\").save(\"caminho\")\n",
    "df.write.parquet(\"caminho\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df\n",
    "df.head(10)\n",
    "\n",
    "# PySpark\n",
    "df.show()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and Pyspark\n",
    "\n",
    "df.columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df.columns = ['a', 'b', 'c']\n",
    "df.rename(columns={'old': 'new'})\n",
    "\n",
    "#PySpark\n",
    "df.toDF('a', 'b', 'c')\n",
    "df.withColumnRenamed('old', 'new')\n",
    "\n",
    "#or\n",
    "df1 = df.selectExpr(\"a\", \n",
    "                    \"b as new_b\",\n",
    "                    \"c as new_c\")\n",
    "#or\n",
    "df = df.select(df['c1'].alias('a'),\n",
    "              df['c2'].alias('b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df.drop('mpg', axis=1)\n",
    "\n",
    "#PySpark\n",
    "df.drop('mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and PySpark\n",
    "\n",
    "df[df.mpg < 30]\n",
    "df[(df.mpg < 30) & (df.cyl > 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas 1/0 = inf\n",
    "df['gpm'] = 1 / df.mpg\n",
    "\n",
    "#PySpark 1/0 = null\n",
    "df.withColumn('gpm', 1 / df.mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df.fillna(0) # <-- Many more options\n",
    "\n",
    "#PySpark\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and PySpark\n",
    "df.groupby(['cyl', 'gear']).agg({'mpg': mean, 'disp': min})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df['col'].value_counts()\n",
    "\n",
    "#PySpark\n",
    "df.groupBy(df['col']).count().orderBy('count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "import numpy as np\n",
    "df['logdisp'] = np.log(df.disp)\n",
    "\n",
    "#PySpark\n",
    "import pyspark.sql.functions as F\n",
    "df.withColumn('logdisp', F.log(df.disp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row conditional statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df['cond'] = df.apply(lambda r: 1 if r.mpg > 20 else 2 if r.cyl == 6 else 3, axis=1)\n",
    "\n",
    "# PySpark\n",
    "df.withColumn('cond', F.when(df.mpg > 20, 1).when(df.cyl == 6, 2).otherwise(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df.dtypes\n",
    "\n",
    "# PySpark\n",
    "df.schema\n",
    "df.printSchema()\n",
    "\n",
    "# you can change type with .astype()\n",
    "df = df.withColumn('Age2', df['Age'].astype(\"float\"))\n",
    "df = df.drop('Age')\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python when required\n",
    "#### Pushing work to the nodes with User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df['disp1'] = df.disp.apply(lambda x: x+1)\n",
    "\n",
    "# PySpark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "fn = F.udf(lambda x: x+1, DoubleType()) #udf user-defined function with he type\n",
    "df.withColumn('disp1', fn(df.disp))\n",
    "\n",
    "#or\n",
    "from pyspark.sql.types import StringType\n",
    "from pysparl.sql.functions import udf\n",
    "\n",
    "yearssinceminor = udf(lambda age: age - 16)\n",
    "new_df = df.withColumn(\"yearssinceminor\", yearssinceminor(df.age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge/Join dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "left.merge(right, on='key')\n",
    "left.merg(right, left_on='a', right_on='b')\n",
    "\n",
    "#if you want to join and your left and right keys has a little bit different name\n",
    "# PySpark\n",
    "left.join(right, on='key')\n",
    "left.join(right, left.a == right.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'], aggfunc=np.sum)\n",
    "\n",
    "# PySpark\n",
    "pd.groubBy('A', 'B').pivot('C').sum('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df.describe()\n",
    "\n",
    "# PySpark\n",
    "df.describe().show() #(only count, mean, stddev, min, max)\n",
    "# quartiles\n",
    "df.selectExpr(\n",
    "        \"percentile_approx(mpg, array(.25, .5, .75)) as mpg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "df.select([mean('age'), min('age'), max('age')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas \n",
    "df.hist()\n",
    "\n",
    "#PySpark\n",
    "df.sample(False, 0.1).toPandas().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a temporary table from our spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark \n",
    "df.createOrReplaceTempView('foo')\n",
    "df2 =  spark.sql('select * from foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a dataframe to a permanet managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"foo2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select one, two from foo sort by three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SQL to create a python spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\" select * from diabetes where diabetes = 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure to\n",
    "- Use pyspark.sql.functions and other built in funcions\n",
    "- Use the same version of python and packages on cluster as driver\n",
    "- Check out the UI at http://localhost:4040/\n",
    "- Learn about SSH por forwarding\n",
    "- Check out Spark MLlib\n",
    "- RTFM: https://spark.apache.org/docs/latest\n",
    "\n",
    "## Things not to do\n",
    "- Try to iterate through rows\n",
    "- Hard code a master in your driver\n",
    "    - Use spark-submit for that\n",
    "- df.toPandas().head()\n",
    "    - instead do: df.limit(5).toPandas()\n",
    "\n",
    "## If things go wrong\n",
    "- Google it\n",
    "- Search/Ask stack Overflow (tag apache-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
